---
title: "Celestial Object Classification"
author: 
  - Xiaoyang Wang
  - Ziang Zeng
output: 
  beamer_presentation:
    # toc: true
    slide_level: 2
    theme: "Madrid"
    # colortheme: "whale"
    citation_package: natbib
bibliography: ./references.bib
biblio-style: unsrt
biblio-title: References
header-includes:
  - \usepackage{bm}
  - \usepackage{booktabs}
#   - \setbeamertemplate{bibliography item}[text]
---



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,
                      message = F,
                      warning = F)
library(reticulate)
```

```{r}
library(ggthemes)
library(tidyverse)
library(kableExtra)

mytheme <- theme(plot.title=element_text(face="bold.italic",
                                         size="14", color="brown"),
                 axis.title=element_text(face="bold.italic",
                                         size=10, color="brown"),
                 axis.text=element_text(face="bold", size=9,
                                        color="darkblue"),
                 panel.background=element_rect(fill="white",
                                               color="darkblue"),
                 panel.grid.major.y=element_line(color="grey",
                                                 linetype=1),
                 panel.grid.minor.y=element_line(color="grey",
                                                 linetype=2),
                 panel.grid.minor.x=element_blank(),
                 legend.position="right") 
```


## Outline

\tableofcontents

# Astronomical Challenge
Classifying celestial objects into stars, galaxies or quasars using their spectral characteristics.

# Data & Preprocessing
## Image of the celestial objects

\begin{figure}
\centering
\begin{minipage}{0.3\textwidth}
\centering
\includegraphics[width=\linewidth]{../data/GALAXY/GALAXY_1.jpg}
\caption{Galaxy}
\end{minipage}\hfill
\begin{minipage}{0.3\textwidth}
\centering
\includegraphics[width=\linewidth]{../data/STAR/STAR_1.jpg}
\caption{Star}
\end{minipage}\hfill
\begin{minipage}{0.3\textwidth}
\centering
\includegraphics[width=\linewidth]{../data/QSO/QSO_1.jpg}
\caption{Qusar}
\end{minipage}
\end{figure}

## Image of the spectra

\begin{figure}
\centering
\begin{minipage}{0.3\textwidth}
\centering
\includegraphics[width=\linewidth]{../data/GALAXY_spec/GALAXY_spectrum_1.jpg}
\caption{Galaxy Spec}
\end{minipage}\hfill
\begin{minipage}{0.3\textwidth}
\centering
\includegraphics[width=\linewidth]{../data/STAR_spec/STAR_spectrum_1.jpg}
\caption{Star Spec}
\end{minipage}\hfill
\begin{minipage}{0.3\textwidth}
\centering
\includegraphics[width=\linewidth]{../data/QSO_spec/QSO_spectrum_1.jpg}
\caption{Qusar Spec}
\end{minipage}
\end{figure}


## Metadata
```{r results='asis'}
df = read.csv("../data/metadata/clean_data.csv")[,-20]
vars = colnames(df)[c(-1, -18, -19)]
explanations = c("Right Ascension angle (at J2000 epoch)",
                 "Declination angle (at J2000 epoch)",
                 "Ultraviolet filter",
                 "Green filter",
                 "Red filter",
                 "Near Infrared filter",
                 "Infrared filter",
                 "Run Number",
                 "Rerun Number",
                 "Camera column",
                 "Field number",
                 "Unique ID used for optical spectroscopic objects",
                 "Object class",
                 "Redshift value based on the increase in wavelength",
                 "Plate",
                 "Modified Julian Date")

tab_exp = data.frame(vars, explanations)

kable(tab_exp, format = "latex", 
      booktabs = TRUE,
      caption = "Metadata of the celestial objects") %>%
  kable_styling(full_width = F, position = "center", font_size = 7)
```

## EDA
- Missing Values: 3

- Samples for each catagory: 33333

# Methodology
## Meta Data
- Explanatory Variables: u, g, r, i, z, redshift

- Response Variable: class
- STAR: 0
- GALAXY: 1
- QSO: 2

- kNN: k = 3

- Decision Tree

- Gini impurity, 
- (para)

- Logistic Regression

- C:0.01
- penalty: l2
- iteration:2000

## Images

\begin{columns}[T] 

\begin{column}{0.5\textwidth}
\includegraphics[width=0.8\textwidth]{../data/simple_cnn_visualization.png}
\end{column}

\begin{column}{0.5\textwidth} 
\begin{itemize}
\item Structure:
\begin{itemize}
\item 2 layers of convolution and 1 maxpooling
\item 3 layers of full connecting 
\item output: \(\vec{y}=(y_1,y_2,y_3)\) \(y_{pred}=argmax_i\{\vec y\}\)\\
Probability through softmax \(P(y = j \mid \mathbf{z}) = \frac{e^{z_j}}{\sum_{k=1}^3 e^{z_k}}\)
\end{itemize}
\item Training:  

SGD with different momentum, Adam, 10 epoch, batch size 64,lr 0.001

\end{itemize}

\end{column}

\end{columns}


## Voting Classifier
\begin{columns}[T] 

\begin{column}{0.5\textwidth}
\includegraphics[width=0.8\textwidth]{./images/voting.png}
\end{column}

\begin{column}{0.5\textwidth} 
\begin{itemize}
\item Soft Voting:
\begin{itemize}
\item Models \(\{C_1,\cdots C_n\}\) 
\item For a given inputs, \(C_i\) has a prediction\(P_i(y_j | x)\) 
\item The predict probabilities for voting classifier \(P(y_j | x) = \frac{1}{m} \sum_{i=1}^m P_i(y_j | x)\)
\item The prediction \( p(x) = \arg \max_{y_j} P(y_j | x)\)
\end{itemize}
\item Hard Voting:
\begin{itemize}
\item \(p(x) = \text{mode}(p_1(x), p_2(x), \ldots, p_m(x))\), mode identify the most frequent one
\end{itemize}
\end{itemize}

\end{column}

\end{columns}
\begin{itemize}
\item Construction:  

The candidate models are KNN, Logistic Regression, Classification Tree, CNN for image and CNN for spectrum image

\end{itemize}
# Results
## Metadata

```{r}
library(rpart)
library(rpart.plot)
df_train <- read.csv('../data/metadata/train_metadata.csv')
r_tree <- rpart(formula = class~.,data = df_train)
rpart.plot(r_tree)
```


## Metadata
```{python results='hide'}
from sklearn.ensemble import VotingClassifier

from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

from sklearn import preprocessing
import pandas as pd

# Load dataset
df_train = pd.read_csv("../data/metadata/train_metadata.csv")
df_test = pd.read_csv("../data/metadata/test_metadata.csv")
X_train, y_train = df_train.iloc[:, 0:6], df_train["class"]

X_test, y_test = df_test.iloc[:, 0:6], df_test["class"]

scaler = preprocessing.StandardScaler().fit(X_train)
X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)

# Instantiate the individual classifiers
knn_clf = KNeighborsClassifier(n_neighbors=3)
tree_clf = DecisionTreeClassifier()
lr_clf = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=5000)


knn_clf.fit(X_train, y_train)
tree_clf.fit(X_train, y_train)
lr_clf.fit(X_train, y_train)

knn_pred = knn_clf.predict(X_test)
tree_pred = tree_clf.predict(X_test)
lr_pred = lr_clf.predict(X_test)
```

```{r}
library(caret)
trainData <- read.csv("../data/metadata/train_metadata.csv")
testData <- read.csv("../data/metadata/test_metadata.csv")
```


```{r}
cm_tree <- confusionMatrix(as.factor(py$tree_pred), as.factor(testData$class))
cm_table <- as.table(cm_tree$table)
cm_df <- as.data.frame(cm_table)
colnames(cm_df) <- c('Reference', 'Prediction', 'Freq')

p1 <- ggplot(cm_df, aes(x = Reference, y = Prediction, fill = Freq)) +
  geom_tile(color = "white") +
  scale_fill_gradient(low = "white", high = "royalblue") +
  geom_text(aes(label = Freq), vjust = 1) +
  labs(title = "Tree", x = "Actual Class", y = "Predicted Class") +
  mytheme

# p1
```

```{r}
cm_knn <- confusionMatrix(as.factor(py$knn_pred), as.factor(testData$class))
cm_table <- as.table(cm_knn$table)
cm_df <- as.data.frame(cm_table)
colnames(cm_df) <- c('Reference', 'Prediction', 'Freq')

p2 <- ggplot(cm_df, aes(x = Reference, y = Prediction, fill = Freq)) +
  geom_tile(color = "white") +
  scale_fill_gradient(low = "white", high = "royalblue") +
  geom_text(aes(label = Freq), vjust = 1) +
  labs(title = "KNN", x = "Actual Class", y = "Predicted Class") +
  mytheme

# p2
```

```{r results='hide'}
cm_lr <- confusionMatrix(as.factor(py$lr_pred), as.factor(testData$class))
cm_table <- as.table(cm_lr$table)
cm_df <- as.data.frame(cm_table)
colnames(cm_df) <- c('Reference', 'Prediction', 'Freq')

p3 <- ggplot(cm_df, aes(x = Reference, y = Prediction, fill = Freq)) +
  geom_tile(color = "white") +
  scale_fill_gradient(low = "white", high = "royalblue") +
  geom_text(aes(label = Freq), vjust = 1) +
  labs(title = "Logistic Regression", x = "Actual Class", y = "Predicted Class") +
  mytheme
```


```{r fig.cap='Confusion Matrices for Metadata Models', fig.height=5}
library(ggpubr)
ggarrange(p1,p2,p3, ncol=3,
          common.legend = TRUE,  legend="bottom")
```


## Images



## Voting Classifier

```{r}
whole_confusion_matrix <- read.csv("results/pred_results.csv")
cm_vt <- confusionMatrix(as.factor(whole_confusion_matrix$Soft.Voting), as.factor(whole_confusion_matrix$y_test))
cm_table <- as.table(cm_vt$table)
cm_df <- as.data.frame(cm_table)
colnames(cm_df) <- c('Reference', 'Prediction', 'Freq')

p4 <- ggplot(cm_df, aes(x = Reference, y = Prediction, fill = Freq)) +
  geom_tile(color = "white") +
  scale_fill_gradient(low = "white", high = "royalblue") +
  geom_text(aes(label = Freq), vjust = 1) +
  labs(title = "Voting Classifier", x = "Actual Class", y = "Predicted Class") +
  mytheme
p4
```


# Conclusions
```{r results='hide'}
accs = c(cm_knn$overall[1],
         cm_tree$overall[1],
         cm_lr$overall[1],
         0.9391,
         0.9914,
         0.9780)

precisions = rbind(
  cm_knn$byClass[,5],
  cm_tree$byClass[,5],
  cm_lr$byClass[,5],
  rep(0, 3),
  rep(0, 3),
  rep(0, 3)
)

recalls = rbind(
  cm_knn$byClass[,6],
  cm_tree$byClass[,6],
  cm_lr$byClass[,6],
  rep(0, 3),
  rep(0, 3),
  rep(0, 3)
)

f1s = rbind(
  cm_knn$byClass[,7],
  cm_tree$byClass[,7],
  cm_lr$byClass[,7],
  rep(0, 3),
  rep(0, 3),
  rep(0, 3)
)

evals = list(precisions, recalls, f1s)
for (i in 1:3){
  colnames(evals[[i]]) = c("Star", "Galaxy", "Qso")
  evals[[i]] = apply(round(evals[[i]],4)*100, 
                     2,
                     function(x) paste0(x, "%")) %>%
    as.data.frame()
}



eval_df = data.frame(
  Data = c("", "M", "M", "M",
           "IC",
           "IS",
           "M+IC+IS"),
  Model = c("", "kNN", "DT", "LR",
            "CNN", "CNN",
            "VC"),
  Accuracy = c("", paste0(round(accs, 4)*100, "%")),
  Precision0 = c("Star", evals[[1]][,1]),
  Precision = c("Galaxy", evals[[1]][,2]),
  Precision2 = c("Qso", evals[[1]][,3]),
  recall0 = c("Star", evals[[2]][,1]),
  Recall = c("Galaxy", evals[[2]][,2]),
  recall2 = c("Qso", evals[[2]][,3]),
  f10 = c("Star", evals[[3]][,1]),
  F1 = c("Galaxy", evals[[3]][,2]),
  f12 = c("Qso", evals[[3]][,3])
)
colnames(eval_df)[c(4, 6, 7, 9, 10, 12)] = ""
# t(eval_df) %>% as.data.frame()
kable(t(eval_df), format = "latex", 
      booktabs = TRUE,
      linesep = c("", "",
  "\\addlinespace"),
      caption = "Evaluation of Models") %>%
  kable_styling(full_width = F, position = "center", font_size = 7)
```

\begin{table}
\centering
\caption{Evaluation of Models}
\centering
\fontsize{7}{9}\selectfont
\begin{tabular}[t]{llllllll}
\toprule
Data &  & M & M & M & IC & IS & M+IC+IS\\
Model &  & kNN & DT & LR & CNN & CNN & VC\\
\midrule
Accuracy &  & 96.79\% & 97.68\% & 97.04\% & 93.91\% & 99.14\% & 97.8\%\\
\addlinespace
 & Star & 95.59\% & 99.82\% & 95.95\% & 0\% & 0\% & 0\%\\
Precision & Galaxy & 96.01\% & 96.45\% & 96.46\% & 0\% & 0\% & 0\%\\
 & Qso & 98.84\% & 96.8\% & 98.77\% & 0\% & 0\% & 0\%\\
\addlinespace
 & Star & 98.67\% & 99.82\% & 99.59\% & 0\% & 0\% & 0\%\\
Recall & Galaxy & 94.61\% & 96.68\% & 95.06\% & 0\% & 0\% & 0\%\\
 & Qso & 97.13\% & 96.56\% & 96.5\% & 0\% & 0\% & 0\%\\
\addlinespace
 & Star & 97.11\% & 99.82\% & 97.74\% & 0\% & 0\% & 0\%\\
F1 & Galaxy & 95.31\% & 96.56\% & 95.76\% & 0\% & 0\% & 0\%\\
 & Qso & 97.98\% & 96.68\% & 97.63\% & 0\% & 0\% & 0\%\\
\bottomrule
\end{tabular}
\end{table}








